#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Med-GRAG Final: Chinese-LLaMA-2 + LoRA + Qwen-NER + Custom Medical Graph
é€‚é…ç”¨æˆ·è‡ªå®šä¹‰çš„ MedicalGraph ç»“æ„ (Disease, Drug, Food, Check...)
"""
import sys
import os
import json
import re
import fire
import gradio as gr
import torch
from peft import PeftModel
from transformers import GenerationConfig, AutoModelForCausalLM, AutoTokenizer
from openai import OpenAI
from neo4j import GraphDatabase
from utils.prompter import Prompter

# ---------- é…ç½®åŒºåŸŸ ----------
# é˜¿é‡Œäº‘ç™¾ç‚¼ API Key
os.environ["DASHSCOPE_API_KEY"] = os.getenv("DASHSCOPE_API_KEY", "sk-00459b72ffb245e5958c40c595d8ff67")

# Neo4j é…ç½® (ä½¿ç”¨ä½ æä¾›çš„è´¦å·å¯†ç )
NEO4J_URI = "bolt://localhost:7687"
NEO4J_USER = "neo4j"
NEO4J_PWD = "lty20001114"  # ä½ çš„å¯†ç 


# ---------- 1. é˜¿é‡Œäº‘ç™¾ç‚¼ NER æ¨¡å— ----------
class AliyunNERExtractor:
    def __init__(self, api_key=None):
        self.api_key = api_key or os.getenv("DASHSCOPE_API_KEY")
        self.client = OpenAI(
            api_key=self.api_key,
            base_url="https://dashscope.aliyuncs.com/compatible-mode/v1"
        )
        # ç®€åŒ–çš„æœ¬åœ°è¯å…¸ç”¨äºå…œåº•
        self.medical_dict = {'æ„Ÿå†’', 'é«˜è¡€å‹', 'ç³–å°¿ç—…', 'å† å¿ƒç—…', 'èƒƒç‚', 'å¤´ç—›', 'å‘çƒ§', 'å’³å—½'}

    def extract_entities(self, text):
        """æå–å®ä½“"""
        if not self.api_key: return self._local_extract(text)
        try:
            completion = self.client.chat.completions.create(
                model="qwen-plus",
                messages=[
                    {"role": "system",
                     "content": "ä½ æ˜¯ä¸€ä¸ªåŒ»å­¦å®ä½“è¯†åˆ«ä¸“å®¶ã€‚æå–æ–‡æœ¬ä¸­çš„åŒ»å­¦å®ä½“ï¼ˆç–¾ç—…ã€ç—‡çŠ¶ã€è¯å“ã€æ£€æŸ¥ï¼‰ã€‚åªè¿”å›JSONåˆ—è¡¨ï¼Œå¦‚[\"æ„Ÿå†’\"]ã€‚"},
                    {"role": "user", "content": f"æå–å®ä½“ï¼š{text}"}
                ],
                temperature=0.0
            )
            txt = completion.choices, [object Object], message.content.strip()
            clean_text = txt.replace("```json", "").replace("```", "").strip()
            return json.loads(clean_text)
        except Exception as e:
            print(f"NER API Error: {e}")
            return self._local_extract(text)

    def _local_extract(self, text):
        return [w for w in self.medical_dict if w in text]


# ---------- 2. è‡ªå®šä¹‰å›¾è°±æ£€ç´¢å™¨ (é€‚é…ä½ çš„ Schema) ----------
class MedicalGraphRetriever:
    def __init__(self, uri, user, pwd):
        try:
            self.driver = GraphDatabase.driver(uri, auth=(user, pwd))
            print("âœ… Neo4j connected successfully.")
        except Exception as e:
            print(f"âŒ Neo4j connection failed: {e}")
            self.driver = None

    def query_entity_context(self, entity_name):
        """
        é’ˆå¯¹ä½ çš„å›¾è°±ç»“æ„è®¾è®¡çš„å…¨æ–¹ä½æŸ¥è¯¢ã€‚
        å¦‚æœå®ä½“æ˜¯'Disease'ï¼ŒæŸ¥è¯¢å®ƒçš„ç—‡çŠ¶ã€è¯ç‰©ã€å¿Œå£ç­‰ã€‚
        å¦‚æœå®ä½“æ˜¯'Symptom'ï¼ŒæŸ¥è¯¢å¯èƒ½å¯¹åº”çš„ç–¾ç—…ã€‚
        """
        if not self.driver: return ""

        context_parts = []

        with self.driver.session() as session:
            # 1. æŸ¥è¯¢æ˜¯å¦æ˜¯ã€ç–¾ç—…ã€‘èŠ‚ç‚¹ (Disease)
            # è·å–ï¼šç®€ä»‹ã€é¢„é˜²ã€æ˜“æ„Ÿäººç¾¤ã€æ²»æ„ˆç‡ã€ç§‘å®¤
            q_disease_info = """
            MATCH (n:Disease {name: $name})
            RETURN n.desc AS desc, n.prevent AS prevent, n.cause AS cause, 
                   n.easy_get AS easy_get, n.cure_way AS cure_way
            """
            result = session.run(q_disease_info, name=entity_name).single()

            if result:
                info = result
                context_parts.append(f"ã€{entity_name}çš„åŸºæœ¬ä¿¡æ¯ã€‘")
                if info['desc']: context_parts.append(f"ç®€ä»‹ï¼š{info['desc']}")
                if info['cause']: context_parts.append(f"æˆå› ï¼š{info['cause']}")
                if info['prevent']: context_parts.append(f"é¢„é˜²ï¼š{info['prevent']}")
                if info['cure_way']: context_parts.append(f"æ²»ç–—æ–¹å¼ï¼š{info['cure_way']}")

                # 2. æŸ¥è¯¢ç–¾ç—…çš„ã€å…³è”å…³ç³»ã€‘ (æ ¹æ®ä½ çš„ create_graphrels å®šä¹‰)
                # æ¨èåƒ(recommand_eat), å¿Œåƒ(no_eat), å®œåƒ(do_eat),
                # å¸¸ç”¨è¯(common_drug), å¥½è¯„è¯(recommand_drug),
                # æ£€æŸ¥(need_check), ç—‡çŠ¶(has_symptom), å¹¶å‘ç—‡(acompany_with)

                q_rels = """
                MATCH (n:Disease {name: $name})-[r]->(m)
                RETURN type(r) AS type, m.name AS target
                """
                rels = session.run(q_rels, name=entity_name)

                rel_dict = {}
                type_map = {
                    'recommand_eat': 'æ¨èé£Ÿè°±', 'no_eat': 'å¿Œåƒé£Ÿç‰©', 'do_eat': 'å®œåƒé£Ÿç‰©',
                    'common_drug': 'å¸¸ç”¨è¯å“', 'recommand_drug': 'æ¨èè¯å“',
                    'need_check': 'æ‰€éœ€æ£€æŸ¥', 'has_symptom': 'å…¸å‹ç—‡çŠ¶',
                    'acompany_with': 'å¹¶å‘ç—‡', 'belongs_to': 'æ‰€å±ç§‘å®¤'
                }

                for r in rels:
                    t = type_map.get(r['type'], r['type'])
                    if t not in rel_dict: rel_dict[t] = []
                    rel_dict[t].append(r['target'])

                for k, v in rel_dict.items():
                    context_parts.append(f"{k}ï¼š{'ã€'.join(v[:10])}")  # é™åˆ¶æ•°é‡é˜²æ­¢Promptè¿‡é•¿

            # 3. æŸ¥è¯¢æ˜¯å¦æ˜¯ã€ç—‡çŠ¶ã€‘èŠ‚ç‚¹ (Symptom) -> æŸ¥å¯èƒ½æ‚£æœ‰çš„ç–¾ç—…
            q_symptom = """
            MATCH (n:Disease)-[:has_symptom]->(s:Symptom {name: $name})
            RETURN n.name AS disease
            LIMIT 10
            """
            res_sym = session.run(q_symptom, name=entity_name)
            diseases = [r['disease'] for r in res_sym]
            if diseases:
                context_parts.append(f"ã€{entity_name}ã€‘å¯èƒ½æ˜¯ä»¥ä¸‹ç–¾ç—…çš„ç—‡çŠ¶ï¼š{'ã€'.join(diseases)}")

            # 4. æŸ¥è¯¢æ˜¯å¦æ˜¯ã€è¯å“ã€‘èŠ‚ç‚¹ (Drug) -> æŸ¥ä¸»æ²»ç–¾ç—…
            q_drug = """
            MATCH (d:Disease)-[:common_drug|recommand_drug]-(dr:Drug {name: $name})
            RETURN d.name AS disease
            LIMIT 10
            """
            res_drug = session.run(q_drug, name=entity_name)
            treated = [r['disease'] for r in res_drug]
            if treated:
                context_parts.append(f"ã€{entity_name}ã€‘å¸¸ç”¨äºæ²»ç–—ï¼š{'ã€'.join(treated)}")

        return "\n".join(context_parts)


# åˆå§‹åŒ–ç»„ä»¶
ner_extractor = AliyunNERExtractor()
kg_retriever = MedicalGraphRetriever(NEO4J_URI, NEO4J_USER, NEO4J_PWD)

# ---------- 3. æ¨¡å‹æ¨ç†ä¸»é€»è¾‘ ----------
device = "cuda" if torch.cuda.is_available() else "cpu"


def main(
        load_8bit: bool = False,
        base_model: str = "hfl/chinese-llama-2-7b",
        use_lora: bool = True,
        lora_weights: str = "lora-chinese-llama2-med/checkpoint-608",
        prompt_template: str = "med_template",
        gradio: bool = False,
):
    # ---- æ¨¡å‹åŠ è½½ ----
    prompter = Prompter(prompt_template)
    tokenizer = AutoTokenizer.from_pretrained(base_model)
    model = AutoModelForCausalLM.from_pretrained(
        base_model, load_in_8bit=load_8bit, torch_dtype=torch.float16, device_map="auto",
    )
    if use_lora:
        model = PeftModel.from_pretrained(model, lora_weights, torch_dtype=torch.float16)

    model.config.pad_token_id = tokenizer.pad_token_id = 0
    model.config.bos_token_id = 1
    model.config.eos_token_id = 2
    if not load_8bit: model.half().eval()

    @torch.no_grad()
    def evaluate(instruction, input=None, **kwargs):
        # 1. å®ä½“æå–
        question = f"{instruction} {input or ''}".strip()
        entities = ner_extractor.extract_entities(question)
        print(f"ğŸ” [NER]: {entities}")

        # 2. çŸ¥è¯†å›¾è°±æ£€ç´¢ (RAG)
        kg_context = ""
        if entities:
            # å¯¹æ¯ä¸ªå®ä½“è¿›è¡Œæ£€ç´¢ï¼Œæ‹¼æ¥ç»“æœ
            contexts = []
            for ent in entities:
                info = kg_retriever.query_entity_context(ent)
                if info: contexts.append(info)
            kg_context = "\n\n".join(contexts)

        print(f"ğŸ“š [Graph Context]:\n{kg_context[:200]}..." if kg_context else "ğŸ“š [Graph Context]: None")

        # 3. æ„é€  Prompt
        # æ ¸å¿ƒï¼šå°†å›¾è°±çŸ¥è¯†ä½œä¸º Context æ³¨å…¥
        if kg_context:
            input_context = (
                f"ä»¥ä¸‹æ˜¯æ£€ç´¢åˆ°çš„åŒ»å­¦çŸ¥è¯†åº“ä¿¡æ¯ï¼Œè¯·ä¼˜å…ˆå‚è€ƒè¿™äº›ä¿¡æ¯å›ç­”ç”¨æˆ·é—®é¢˜ï¼š\n"
                f"---------------------\n"
                f"{kg_context}\n"
                f"---------------------\n"
                f"ç”¨æˆ·è¡¥å……ä¿¡æ¯ï¼š{input or 'æ— '}"
            )
        else:
            input_context = input or ""

        prompt = prompter.generate_prompt(instruction, input_context)
        inputs = tokenizer(prompt, return_tensors="pt").to(device)

        # 4. ç”Ÿæˆ
        output_ids = model.generate(
            **inputs,
            generation_config=GenerationConfig(
                temperature=0.1, top_p=0.75, top_k=40, num_beams=1, max_new_tokens=512
            )
        )
        response = tokenizer.decode(output_ids, [object Object],, skip_special_tokens = True)
        return prompter.get_response(response)

    # ---- å¯åŠ¨ç•Œé¢ ----
    if gradio:
        gr.Interface(
            fn=evaluate,
            inputs=[gr.Textbox(label="é—®é¢˜"), gr.Textbox(label="è¡¥å……ä¿¡æ¯")],
            outputs=gr.Textbox(label="å›ç­”"),
            title="Med-GRAG System",
            description="Chinese-LLaMA-2 + LoRA + Qwen-NER + Custom Medical Graph"
        ).launch(server_name="0.0.0.0", share=False)
    else:
        # æµ‹è¯•ç”¨ä¾‹
        q = "æˆ‘æœ€è¿‘æ€»æ˜¯å¤´ç—›ï¼Œè€Œä¸”æœ‰ç‚¹é«˜è¡€å‹ï¼Œè¯·é—®é¥®é£Ÿä¸Šè¦æ³¨æ„ä»€ä¹ˆï¼Ÿ"
        print(f"\nQuestion: {q}")
        print(f"Answer: {evaluate(q)}")


if __name__ == "__main__":
    fire.Fire(main)
# med_grag_final.py

# -*- coding: utf-8 -*-
# !/usr/bin/env python3
"""
Med-GRAG Final: Chinese-LLaMA-2 + LoRA + Qwen-NER + Custom Medical Graph
- ä¿®å¤ Runtime Error: å½»åº•ç§»é™¤ bitsandbytes é‡åŒ–ï¼Œæ”¹ç”¨åŸç”Ÿ FP16
- ä¿®å¤ RAGé€»è¾‘: å®ä½“ä¼˜å…ˆçº§ + å¼ºåˆ¶å›ç­”å½“å‰é—®é¢˜ + æé«˜é˜²é‡å¤æƒ©ç½š
"""
import sys
import os
import json
import re
import fire
import gradio as gr
import torch
from peft import PeftModel
from transformers import GenerationConfig, AutoModelForCausalLM, AutoTokenizer
from openai import OpenAI
from neo4j import GraphDatabase

# å¯¼å…¥ utils/prompter.py
try:
    from utils.prompter import Prompter
except ImportError:
    print("âŒ é”™è¯¯: æ— æ³•å¯¼å…¥ utils.prompterã€‚è¯·ç¡®ä¿ utils/prompter.py æ–‡ä»¶å­˜åœ¨ã€‚")
    sys.exit(1)

# ==========================================
# ğŸ›‘ ç¯å¢ƒé…ç½®
# ==========================================

# 1. è§£å†³ OOM çš„ç¢ç‰‡åŒ–é—®é¢˜
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "max_split_size_mb:128"

# 2. é˜¿é‡Œäº‘ç™¾ç‚¼ API Key
os.environ["DASHSCOPE_API_KEY"] = os.getenv("DASHSCOPE_API_KEY", "sk-00459b72ffb245e5958c40c595d8ff67")

# ==========================================
# é…ç½®åŒºåŸŸ
# ==========================================
NEO4J_URI = "bolt://localhost:7687"
NEO4J_USER = "neo4j"
NEO4J_PWD = "lty20001114"

BASE_MODEL_PATH = "chinese-llama-2-7b"
LORA_WEIGHTS_PATH = "lora-chinese-llama2-med/checkpoint-608"


# ---------- 1. é˜¿é‡Œäº‘ç™¾ç‚¼ NER æ¨¡å— (ä¿æŒä¸å˜) ----------
class AliyunNERExtractor:
    def __init__(self, api_key=None):
        self.api_key = api_key or os.getenv("DASHSCOPE_API_KEY")
        if not self.api_key:
            print("âš ï¸ WARN: DASHSCOPE_API_KEY not found. Using local dictionary.")
        self.client = OpenAI(api_key=self.api_key, base_url="https://dashscope.aliyuncs.com/compatible-mode/v1")
        self.medical_dict = {'æ„Ÿå†’', 'é«˜è¡€å‹', 'ç³–å°¿ç—…', 'å† å¿ƒç—…', 'èƒƒç‚', 'å¤´ç—›', 'å‘çƒ§', 'å’³å—½', 'é˜¿å¸åŒ¹æ—', 'å¤±çœ '}

    def extract_entities(self, text):
        if not self.api_key: return self._local_extract(text)
        try:
            completion = self.client.chat.completions.create(
                model="qwen-plus",
                messages=[
                    {"role": "system",
                     "content": "ä½ æ˜¯ä¸€ä¸ªåŒ»å­¦å®ä½“è¯†åˆ«ä¸“å®¶ã€‚æå–æ‰€æœ‰åŒ»å­¦å®ä½“ï¼Œåªè¿”å›JSONæ ¼å¼çš„åˆ—è¡¨ï¼Œå¦‚[\"æ„Ÿå†’\"]ã€‚"},
                    {"role": "user", "content": f"æå–å®ä½“ï¼š{text}"}
                ],
                temperature=0.0
            )
            txt = completion.choices[0].message.content.strip()
            clean_text = txt.replace("```json", "").replace("```", "").strip()
            return json.loads(clean_text)
        except Exception as e:
            print(f"âŒ NER API Error: {e}")
            return self._local_extract(text)

    def _local_extract(self, text):
        return [w for w in self.medical_dict if w in text]


# ---------- 2. è‡ªå®šä¹‰å›¾è°±æ£€ç´¢å™¨ (ä¼˜åŒ–ç‰ˆ) ----------
class MedicalGraphRetriever:
    def __init__(self, uri, user, pwd):
        try:
            self.driver = GraphDatabase.driver(uri, auth=(user, pwd))
            self.driver.verify_connectivity()
            print("âœ… Neo4j connected successfully.")
        except Exception as e:
            print(f"âŒ Neo4j connection failed: {e}")
            self.driver = None

    RELATION_MAP = {
        'recommand_eat': 'æ¨èé£Ÿè°±', 'no_eat': 'å¿Œåƒé£Ÿç‰©', 'do_eat': 'å®œåƒé£Ÿç‰©',
        'common_drug': 'å¸¸ç”¨è¯å“', 'recommand_drug': 'æ¨èè¯å“',
        'need_check': 'æ‰€éœ€æ£€æŸ¥', 'has_symptom': 'å…¸å‹ç—‡çŠ¶',
        'acompany_with': 'å¹¶å‘ç—‡', 'belongs_to': 'æ‰€å±ç§‘å®¤'
    }

    def query_entity_context(self, entity_name):
        if not self.driver: return ""
        context_parts = []

        # è¿‡æ»¤æ‰ä¸€äº›æ— ç”¨çš„é€šç”¨å®ä½“
        stop_words = {'å†…ç§‘', 'å¤–ç§‘', 'åŒ»é™¢', 'åŒ»ç”Ÿ', 'å»ºè®®', 'æ£€æŸ¥'}
        if entity_name in stop_words:
            return ""

        with self.driver.session() as session:
            # 1. ä¼˜å…ˆæŸ¥è¯¢ã€è¯å“ã€‘(Drug)
            q_drug_node = "MATCH (n:Drug {name: $name}) RETURN n.desc AS desc, n.effect AS effect"
            result_drug = session.run(q_drug_node, name=entity_name).data()
            if result_drug:
                info = result_drug[0]
                context_parts.append(f"ã€è¯å“ï¼š{entity_name}ã€‘")
                if info.get('desc'): context_parts.append(f"è¯´æ˜ï¼š{info['desc'][:100]}...")

                q_drug_cure = "MATCH (n:Drug {name: $name})-[:recommand_drug|common_drug]-(d:Disease) RETURN d.name as disease LIMIT 5"
                cures = [r['disease'] for r in session.run(q_drug_cure, name=entity_name)]
                if cures: context_parts.append(f"ä¸»æ²»ç–¾ç—…ï¼š{'ã€'.join(cures)}")

            # 2. æŸ¥è¯¢ã€ç–¾ç—…ã€‘(Disease)
            q_disease_info = """
            MATCH (n:Disease {name: $name})
            RETURN n.desc AS desc, n.prevent AS prevent, n.cause AS cause, n.cure_way AS cure_way
            """
            result_disease = session.run(q_disease_info, name=entity_name).data()
            if result_disease:
                info = result_disease[0]
                context_parts.append(f"ã€ç–¾ç—…ï¼š{entity_name}ã€‘")
                if info.get('desc'): context_parts.append(f"ç®€ä»‹ï¼š{info['desc'][:100]}...")

                # é‡ç‚¹ï¼šé¥®é£Ÿå’Œè¯ç‰©æŸ¥è¯¢
                q_rels = "MATCH (n:Disease {name: $name})-[r]->(m) RETURN type(r) AS type, m.name AS target"
                rels = session.run(q_rels, name=entity_name)

                rel_dict = {}
                for r in rels:
                    t = self.RELATION_MAP.get(r['type'], r['type'])
                    if t not in rel_dict: rel_dict[t] = []
                    rel_dict[t].append(r['target'])

                # ä¼˜å…ˆå±•ç¤ºé¥®é£Ÿå’Œè¯ç‰©ï¼Œå¹¶é™åˆ¶æ•°é‡
                priority_keys = ['å¿Œåƒé£Ÿç‰©', 'å®œåƒé£Ÿç‰©', 'æ¨èé£Ÿè°±', 'å¸¸ç”¨è¯å“', 'æ¨èè¯å“']
                for k in priority_keys:
                    if k in rel_dict:
                        context_parts.append(f"{k}ï¼š{'ã€'.join(rel_dict[k][:5])}")  # é™åˆ¶åªæ˜¾ç¤º5ä¸ª

        # é™åˆ¶æ€» Context é•¿åº¦
        return "\n".join(context_parts[:8])

    # åˆå§‹åŒ–ç»„ä»¶


ner_extractor = AliyunNERExtractor()
kg_retriever = MedicalGraphRetriever(NEO4J_URI, NEO4J_USER, NEO4J_PWD)

# ---------- 3. æ¨¡å‹æ¨ç†ä¸»é€»è¾‘ ----------
device = "cuda" if torch.cuda.is_available() else "cpu"


def main(
        base_model: str = BASE_MODEL_PATH,
        use_lora: bool = True,
        lora_weights: str = LORA_WEIGHTS_PATH,
        prompt_template: str = "med_template",
        gradio: bool = False,
):
    prompter = Prompter(prompt_template)

    # å°è¯•åŠ è½½ LLM
    actual_evaluate = None
    try:
        print(f"â³ æ­£åœ¨åŠ è½½æ¨¡å‹ (FP16æ¨¡å¼)...")

        tokenizer = AutoTokenizer.from_pretrained(
            base_model,
            trust_remote_code=True
        )

        # âš ï¸ å…³é”®ä¿®å¤ï¼šä½¿ç”¨ torch.float16 åŠ è½½ï¼Œè§£å†³ Quantization shape é”™è¯¯
        model = AutoModelForCausalLM.from_pretrained(
            base_model,
            torch_dtype=torch.float16,
            device_map="auto",
            trust_remote_code=True
        )

        if use_lora:
            print(f"â³ æ­£åœ¨åŠ è½½ LoRA: {lora_weights}")
            model = PeftModel.from_pretrained(
                model,
                lora_weights,
                torch_dtype=torch.float16
            )

        model.config.pad_token_id = tokenizer.pad_token_id = 0
        model.config.bos_token_id = 1
        model.config.eos_token_id = 2

        model.eval()
        print("âœ… LLM åŠ è½½æˆåŠŸ (FP16 Native)!")

    except Exception as e:
        print(f"âŒ Error loading LLM: {e}")

        # Mock å‡½æ•° (å…œåº•)
        def mock_evaluate(instruction, history, **kwargs):
            formatted_history = "\n".join([f"User: {q}\nBot: {a}" for q, a in history])
            full_context = f"{formatted_history}\nç”¨æˆ·ï¼š{instruction}"
            entities = ner_extractor.extract_entities(full_context)
            kg_context = ""
            if entities:
                kg_context = kg_retriever.query_entity_context(entities[0])

            if kg_context:
                mock_answer = f"ã€æ¨¡æ‹ŸRAGå›ç­” (æ¨¡å‹åŠ è½½å¤±è´¥)ã€‘æ ¹æ®'{entities[0]}'ï¼Œæ£€ç´¢åˆ°ä¿¡æ¯ã€‚è¯·é—®æ‚¨è¿˜æœ‰å…¶ä»–ç—‡çŠ¶å—ï¼Ÿ"
            else:
                mock_answer = f"ã€æ¨¡æ‹Ÿå›ç­” (æ¨¡å‹åŠ è½½å¤±è´¥)ã€‘æ‚¨æåˆ°äº†'{instruction}'ã€‚è¯·æ‚¨è¯¦ç»†æè¿°ä¸€ä¸‹æ‚¨æœ€è¿‘æ„Ÿè§‰å¦‚ä½•ï¼Ÿ"
            return mock_answer, history + [(instruction, mock_answer)]

        actual_evaluate = mock_evaluate
        print("âš ï¸ å·²åˆ‡æ¢åˆ° Mock æ¨¡å¼ã€‚")

    # å†å²è®°å½•æ ¼å¼åŒ–
    def format_history(history):
        formatted = []
        for user_q, bot_a in history:
            formatted.append(f"ç”¨æˆ·ï¼š{user_q}")
            formatted.append(f"åŠ©æ‰‹ï¼š{bot_a}")
        return "\n".join(formatted)

    @torch.no_grad()
    def evaluate(instruction, history, **kwargs):
        # å¦‚æœæ¨¡å‹åŠ è½½å¤±è´¥ï¼Œè°ƒç”¨ mock
        if actual_evaluate:
            return actual_evaluate(instruction, history, **kwargs)

        if device == "cuda":
            torch.cuda.empty_cache()

        current_question = instruction.strip()

        # A. æå–å®ä½“ï¼šåŒºåˆ†â€œå½“å‰é—®é¢˜å®ä½“â€å’Œâ€œå†å²å®ä½“â€
        current_entities = ner_extractor.extract_entities(current_question)
        formatted_history_text = "\n".join([f"{u} {b}" for u, b in history])
        history_entities = ner_extractor.extract_entities(formatted_history_text)

        # åˆå¹¶å®ä½“ï¼šå½“å‰å®ä½“ä¼˜å…ˆï¼Œç„¶åæ˜¯å†å²ä¸­æœªè¢«å½“å‰é—®é¢˜æåŠçš„å®ä½“
        final_search_entities = current_entities + [e for e in history_entities if e not in current_entities]

        print(f"ğŸ” [Current Entities]: {current_entities}")
        print(f"ğŸ” [History Entities]: {history_entities}")

        # B. çŸ¥è¯†å›¾è°±æ£€ç´¢
        kg_context = ""
        if final_search_entities:
            contexts = []
            # åªæŸ¥å‰ 3 ä¸ªå®ä½“
            for ent in final_search_entities[:3]:
                info = kg_retriever.query_entity_context(ent)
                if info: contexts.append(info)
            kg_context = "\n\n".join(contexts)

        print(f"ğŸ“š [Graph Context]:\n{kg_context[:100]}..." if kg_context else "ğŸ“š [Graph Context]: None")

        # C. æ„é€  Prompt (å¼ºåŒ–æŒ‡ä»¤ï¼Œé˜²æ­¢å¤è¯»)
        formatted_history = format_history(history)

        # âš ï¸ å¼ºåŒ–ç³»ç»ŸæŒ‡ä»¤
        system_prompt = (
            "ä½ æ˜¯ä¸€åä¸“ä¸šçš„åŒ»ç”Ÿã€‚è¯·åŸºäºã€çŸ¥è¯†åº“ä¿¡æ¯ã€‘å›ç­”ç”¨æˆ·çš„ã€å½“å‰é—®é¢˜ã€‘ã€‚\n"
            "æ³¨æ„ï¼š\n"
            "1. å¦‚æœç”¨æˆ·é—®é¥®é£Ÿï¼Œå¿…é¡»å›ç­”å¿Œå£å’Œå®œåƒé£Ÿç‰©ã€‚\n"
            "2. å¦‚æœç”¨æˆ·é—®è¯ç‰©ï¼Œå¿…é¡»åŸºäºçŸ¥è¯†åº“è¯´æ˜è¯ç‰©ä½œç”¨å’Œé€‚åº”ç—‡ã€‚\n"
            "3. **ç¦æ­¢é‡å¤ä¹‹å‰çš„å›ç­”**ï¼Œå¿…é¡»é’ˆå¯¹ã€å½“å‰é—®é¢˜ã€‘è¿›è¡Œæ–°ä¸€è½®çš„è§£ç­”ã€‚"
        )

        input_context = (
            f"ã€çŸ¥è¯†åº“ä¿¡æ¯ã€‘:\n{kg_context or 'æš‚æ— å…·ä½“æ•°æ®ï¼Œè¯·ä¾æ®å¸¸è¯†å›ç­”'}\n\n"
            f"ã€å†å²å¯¹è¯ã€‘:\n{formatted_history}\n\n"
            f"ã€å½“å‰é—®é¢˜ã€‘:\n{current_question}"
        )

        prompt = prompter.generate_prompt(
            instruction=system_prompt,
            input=input_context
        )

        # D. ç”Ÿæˆ
        inputs = tokenizer(prompt, return_tensors="pt").to(device)
        inputs.pop("token_type_ids", None)

        output_ids = model.generate(
            **inputs,
            generation_config=GenerationConfig(
                temperature=0.2,  # ç¨å¾®æé«˜ï¼Œé¿å…æ­»æ¿
                top_p=0.8,
                top_k=40,
                num_beams=1,
                max_new_tokens=512,
                repetition_penalty=1.2  # å…³é”®ï¼æé«˜é˜²å¤è¯»æƒ©ç½š
            )
        )
        response = tokenizer.decode(output_ids[0], skip_special_tokens=True)
        final_answer = prompter.get_response(response)

        # 5. æ›´æ–°å†å²è®°å½•
        new_history = history + [(current_question, final_answer)]
        return final_answer, new_history

    # ---- Gradio ----
    if gradio:
        with gr.Blocks(title="Med-GRAG System") as demo:
            gr.Markdown("<h1>ğŸ§  Med-GRAG å¤šè½®å¯¹è¯ç³»ç»Ÿ</h1>")

            history_state = gr.State([])
            chatbot = gr.Chatbot(label="å¯¹è¯è®°å½•", height=450)
            msg = gr.Textbox(label="è¾“å…¥ï¼š")
            clear = gr.Button("æ¸…é™¤")

            def respond(message, chat_history_list):
                final_answer, new_history = evaluate(message, chat_history_list)
                return new_history, "", new_history

            msg.submit(respond, [msg, history_state], [chatbot, msg, history_state], queue=False)
            clear.click(lambda: ([], []), None, [chatbot, history_state], queue=False)

        demo.launch(server_name="0.0.0.0", share=False)
    else:
        # å‘½ä»¤è¡Œæµ‹è¯•
        history = []
        q1 = "æˆ‘æœ€è¿‘æ€»æ˜¯å¤´ç—›ï¼Œè€Œä¸”æœ‰ç‚¹é«˜è¡€å‹ï¼Œè¯·é—®é¥®é£Ÿä¸Šè¦æ³¨æ„ä»€ä¹ˆï¼Ÿ"
        print(f"\n======== Round 1 ========\nQ1: {q1}")
        a1, history = evaluate(q1, history)
        print(f"A1: {a1}")

        q2 = "é‚£åƒé˜¿å¸åŒ¹æ—å¯ä»¥å—ï¼Ÿ"
        print(f"\n======== Round 2 ========\nQ2: {q2}")
        a2, history = evaluate(q2, history)
        print(f"A2: {a2}")


if __name__ == "__main__":
    fire.Fire(main)
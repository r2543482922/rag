# -*- coding: utf-8 -*-
# -*- coding: utf-8 -*-
# med_grag_multiturn.py

# -*- coding: utf-8 -*-
# !/usr/bin/env python3
"""
Med-GRAG Final: Chinese-LLaMA-2 + LoRA + Qwen-NER + Custom Medical Graph
é€‚é…ç”¨æˆ·è‡ªå®šä¹‰çš„ MedicalGraph ç»“æ„ (Disease, Drug, Food, Check...)
å¹¶å®ç°å¤šè½®å¯¹è¯åŠŸèƒ½ (Multiturn)
"""
import os
import json
import fire
import gradio as gr
import torch
from peft import PeftModel
from transformers import GenerationConfig, AutoModelForCausalLM, AutoTokenizer
from openai import OpenAI
from neo4j import GraphDatabase
# å¯¼å…¥ prompter
from utils.å¤šè½®.prompter_plus import Prompter

# ---------- é…ç½®åŒºåŸŸ ----------
# é˜¿é‡Œäº‘ç™¾ç‚¼ API Key (è¯·ç¡®ä¿ç¯å¢ƒå˜é‡å·²è®¾ç½®æˆ–åœ¨æ­¤å¤„æ‰‹åŠ¨å¡«å†™)
os.environ["DASHSCOPE_API_KEY"] = os.getenv("DASHSCOPE_API_KEY",
                                            "sk-00459b72ffb245e5958c40c595d8ff67")  # âš ï¸ è¿™æ˜¯ä¸€ä¸ªç¤ºä¾‹å€¼ï¼Œè¯·æ›¿æ¢ä¸ºæ‚¨çš„çœŸå® Key

# Neo4j é…ç½® (ä½¿ç”¨ä½ æä¾›çš„è´¦å·å¯†ç )
NEO4J_URI = "bolt://localhost:7687"
NEO4J_USER = "neo4j"
NEO4J_PWD = "lty20001114"  # âš ï¸ è¿™æ˜¯ä¸€ä¸ªç¤ºä¾‹å€¼ï¼Œè¯·æ›¿æ¢ä¸ºæ‚¨çš„çœŸå®å¯†ç 

# æ¨¡å‹è·¯å¾„é…ç½®
# è¯·ç¡®ä¿è¿™äº›è·¯å¾„æŒ‡å‘ä½ æœ¬åœ°çš„æ¨¡å‹æ–‡ä»¶
BASE_MODEL_PATH = "chinese-llama-2-7b"
LORA_WEIGHTS_PATH = "lora-chinese-llama2-med/checkpoint-608"


# ---------- 1. é˜¿é‡Œäº‘ç™¾ç‚¼ NER æ¨¡å— ----------
class AliyunNERExtractor:
    def __init__(self, api_key=None):
        self.api_key = api_key or os.getenv("DASHSCOPE_API_KEY")
        if not self.api_key:
            print("âš ï¸ WARN: DASHSCOPE_API_KEY not found. Using local dictionary for NER.")

        self.client = OpenAI(
            api_key=self.api_key,
            base_url="https://dashscope.aliyuncs.com/compatible-mode/v1"
        )
        # ç®€åŒ–çš„æœ¬åœ°è¯å…¸ç”¨äºå…œåº•
        self.medical_dict = {'æ„Ÿå†’', 'é«˜è¡€å‹', 'ç³–å°¿ç—…', 'å† å¿ƒç—…', 'èƒƒç‚', 'å¤´ç—›', 'å‘çƒ§', 'å’³å—½', 'é˜¿å¸åŒ¹æ—'}

    def extract_entities(self, text):
        """æå–å®ä½“"""
        if not self.api_key: return self._local_extract(text)
        try:
            completion = self.client.chat.completions.create(
                model="qwen-plus",
                messages=[
                    {"role": "system",
                     "content": "ä½ æ˜¯ä¸€ä¸ªåŒ»å­¦å®ä½“è¯†åˆ«ä¸“å®¶ã€‚ä»ç”¨æˆ·æä¾›çš„æ–‡æœ¬ä¸­ï¼Œæå–æ‰€æœ‰å¯èƒ½çš„åŒ»å­¦å®ä½“ï¼ˆç–¾ç—…ã€ç—‡çŠ¶ã€è¯å“ã€æ£€æŸ¥ã€é£Ÿç‰©ç­‰ï¼‰ã€‚åªè¿”å›JSONæ ¼å¼çš„åˆ—è¡¨ï¼Œå¦‚[\"æ„Ÿå†’\", \"å¤´ç—›\"]ã€‚"},
                    {"role": "user", "content": f"æå–å®ä½“ï¼š{text}"}
                ],
                temperature=0.0
            )
            txt = completion.choices[0].message.content.strip()
            # æ¸…ç†å¯èƒ½çš„ markdown æ ¼å¼
            clean_text = txt.replace("```json", "").replace("```", "").strip()
            return json.loads(clean_text)
        except Exception as e:
            print(f"âŒ NER API Error, falling back to local dictionary: {e}")
            return self._local_extract(text)

    def _local_extract(self, text):
        # ç®€å•åŒ¹é…æœ¬åœ°è¯å…¸
        return [w for w in self.medical_dict if w in text]


# ---------- 2. è‡ªå®šä¹‰å›¾è°±æ£€ç´¢å™¨ (é€‚é…ä½ çš„ Schema) ----------
class MedicalGraphRetriever:
    def __init__(self, uri, user, pwd):
        try:
            self.driver = GraphDatabase.driver(uri, auth=(user, pwd))
            self.driver.verify_connectivity()
            print("âœ… Neo4j connected successfully.")
        except Exception as e:
            print(f"âŒ Neo4j connection failed: {e}")
            self.driver = None

    def query_entity_context(self, entity_name):
        """
        é’ˆå¯¹å›¾è°±ç»“æ„è®¾è®¡çš„å…¨æ–¹ä½æŸ¥è¯¢ã€‚
        """
        if not self.driver: return ""

        context_parts = []

        # å®šä¹‰å…³ç³»æ˜ å°„è¡¨
        type_map = {
            'recommand_eat': 'æ¨èé£Ÿè°±', 'no_eat': 'å¿Œåƒé£Ÿç‰©', 'do_eat': 'å®œåƒé£Ÿç‰©',
            'common_drug': 'å¸¸ç”¨è¯å“', 'recommand_drug': 'æ¨èè¯å“',
            'need_check': 'æ‰€éœ€æ£€æŸ¥', 'has_symptom': 'å…¸å‹ç—‡çŠ¶',
            'acompany_with': 'å¹¶å‘ç—‡', 'belongs_to': 'æ‰€å±ç§‘å®¤'
        }

        with self.driver.session() as session:
            # 1. æŸ¥è¯¢æ˜¯å¦æ˜¯ã€ç–¾ç—…ã€‘èŠ‚ç‚¹ (Disease)
            # è·å–ï¼šç®€ä»‹ã€æˆå› ã€é¢„é˜²ã€æ²»ç–—æ–¹å¼
            q_disease_info = """
            MATCH (n:Disease {name: $name})
            RETURN n.desc AS desc, n.prevent AS prevent, n.cause AS cause, 
                   n.easy_get AS easy_get, n.cure_way AS cure_way
            """
            result = session.run(q_disease_info, name=entity_name).single()

            if result:
                info = result
                context_parts.append(f"ã€ç–¾ç—…ï¼š{entity_name}çš„åŸºæœ¬ä¿¡æ¯ã€‘")
                if info['desc']: context_parts.append(f"ç®€ä»‹ï¼š{info['desc']}")
                if info['cause']: context_parts.append(f"æˆå› ï¼š{info['cause']}")
                if info['prevent']: context_parts.append(f"é¢„é˜²ï¼š{info['prevent']}")
                if info['cure_way']: context_parts.append(f"æ²»ç–—æ–¹å¼ï¼š{info['cure_way']}")

                # æŸ¥è¯¢ç–¾ç—…çš„ã€å…³è”å…³ç³»ã€‘
                q_rels = """
                MATCH (n:Disease {name: $name})-[r]->(m)
                RETURN type(r) AS type, m.name AS target
                """
                rels = session.run(q_rels, name=entity_name)

                rel_dict = {}
                for r in rels:
                    t = type_map.get(r['type'], r['type'])
                    if t not in rel_dict: rel_dict[t] = []
                    rel_dict[t].append(r['target'])

                for k, v in rel_dict.items():
                    context_parts.append(f"{k}ï¼š{'ã€'.join(v[:10])}")

            # 3. æŸ¥è¯¢æ˜¯å¦æ˜¯ã€ç—‡çŠ¶ã€‘èŠ‚ç‚¹ (Symptom) -> æŸ¥å¯èƒ½æ‚£æœ‰çš„ç–¾ç—…
            q_symptom = """
            MATCH (n:Disease)-[:has_symptom]->(s {name: $name})
            RETURN n.name AS disease
            LIMIT 10
            """
            res_sym = session.run(q_symptom, name=entity_name)
            diseases = [r['disease'] for r in res_sym]
            if diseases:
                context_parts.append(f"ã€ç—‡çŠ¶ï¼š{entity_name}ã€‘å¯èƒ½æ˜¯ä»¥ä¸‹ç–¾ç—…çš„ç—‡çŠ¶ï¼š{'ã€'.join(diseases)}")

            # 4. æŸ¥è¯¢æ˜¯å¦æ˜¯ã€è¯å“ã€‘èŠ‚ç‚¹ (Drug) -> æŸ¥ä¸»æ²»ç–¾ç—…
            q_drug = """
            MATCH (d:Disease)-[r]-(dr)
            WHERE type(r) IN ['common_drug', 'recommand_drug'] AND dr.name = $name
            RETURN d.name AS disease
            LIMIT 10
            """
            res_drug = session.run(q_drug, name=entity_name)
            treated = [r['disease'] for r in res_drug]
            if treated:
                context_parts.append(f"ã€è¯å“ï¼š{entity_name}ã€‘å¸¸ç”¨äºæ²»ç–—ï¼š{'ã€'.join(treated)}")

        return "\n".join(context_parts)


# åˆå§‹åŒ–ç»„ä»¶
ner_extractor = AliyunNERExtractor()
kg_retriever = MedicalGraphRetriever(NEO4J_URI, NEO4J_USER, NEO4J_PWD)

# ---------- 3. æ¨¡å‹æ¨ç†ä¸»é€»è¾‘ ----------
device = "cuda" if torch.cuda.is_available() else "cpu"


def main(
        load_8bit: bool = False,
        base_model: str = BASE_MODEL_PATH,
        use_lora: bool = True,
        lora_weights: str = LORA_WEIGHTS_PATH,
        prompt_template: str = "med_template",
        gradio: bool = False,
):
    # ---- æ¨¡å‹åŠ è½½ ----
    prompter = Prompter(prompt_template)

    # å°è¯•åŠ è½½ LoRA æ¨¡å‹
    try:
        tokenizer = AutoTokenizer.from_pretrained(base_model)
        model = AutoModelForCausalLM.from_pretrained(
            base_model, load_in_8bit=load_8bit, torch_dtype=torch.float16, device_map="auto",
        )
        if use_lora:
            model = PeftModel.from_pretrained(model, lora_weights, torch_dtype=torch.float16)

        model.config.pad_token_id = tokenizer.pad_token_id = 0
        model.config.bos_token_id = 1
        model.config.eos_token_id = 2
        if not load_8bit: model.half().eval()
        print("âœ… LLM loaded successfully.")
    except Exception as e:
        print(f"âŒ Error loading LLM/LoRA weights: {e}")

        # åœ¨æ— æ³•åŠ è½½æ¨¡å‹æ—¶ï¼Œä½¿ç”¨ä¸€ä¸ªå“‘å‡½æ•°ï¼ˆMock Functionï¼‰è¿›è¡Œæµ‹è¯•
        def mock_evaluate(instruction, history, **kwargs):
            formatted_history = "\n".join([f"User: {q}\nBot: {a}" for q, a in history])
            print(f"--- Mock Evaluation ---\nHistory:\n{formatted_history}\nQuestion: {instruction}")

            # ä½¿ç”¨ NER/KG æ£€ç´¢ç»“æœè¿›è¡Œç®€å•çš„æ¨¡æ‹Ÿå›å¤
            full_context = f"{formatted_history}\nç”¨æˆ·ï¼š{instruction}"
            entities = ner_extractor.extract_entities(full_context)
            kg_context = ""
            if entities:
                kg_context = kg_retriever.query_entity_context(entities[0])  # åªæŸ¥ç¬¬ä¸€ä¸ª

            if kg_context:
                mock_answer = f"ã€æ¨¡æ‹ŸRAGå›ç­”ã€‘æ ¹æ®æ‚¨æåˆ°çš„'{entities[0]}'ï¼Œæˆ‘ä»¬ä»çŸ¥è¯†åº“ä¸­æ£€ç´¢åˆ°ä»¥ä¸‹ä¿¡æ¯ï¼š\n{kg_context.splitlines()[0]}...\nè¯·é—®æ‚¨è¿˜æœ‰å…¶ä»–ç—‡çŠ¶å—ï¼Ÿ"
            else:
                mock_answer = f"ã€æ¨¡æ‹Ÿå›ç­”ã€‘æ‚¨æåˆ°äº†'{instruction}'ã€‚æˆ‘æ²¡æœ‰æ‰¾åˆ°ç›¸å…³çŸ¥è¯†ã€‚è¯·æ‚¨è¯¦ç»†æè¿°ä¸€ä¸‹æ‚¨æœ€è¿‘æ„Ÿè§‰å¦‚ä½•ï¼Ÿ"

            new_history = history + [(instruction, mock_answer)]
            return mock_answer, new_history

        evaluate = mock_evaluate
        print("âš ï¸ Falling back to Mock Evaluation. Cannot run full LLM RAG without model weights.")

    # æ ¼å¼åŒ–å†å²è®°å½•ï¼Œä½œä¸º LLM çš„ Context
    def format_history(history):
        formatted = []
        for user_q, bot_a in history:
            formatted.append(f"ç”¨æˆ·ï¼š{user_q}")
            formatted.append(f"åŠ©æ‰‹ï¼š{bot_a}")
        return "\n".join(formatted)

    @torch.no_grad()
    def evaluate(instruction, history, **kwargs):  # ğŸ‘ˆ å¤šè½®å¯¹è¯å…¥å£

        current_question = instruction.strip()
        formatted_history = format_history(history)

        # å°†å†å²å¯¹è¯å’Œå½“å‰é—®é¢˜æ‹¼æ¥ï¼Œä½œä¸ºå®Œæ•´çš„ä¸Šä¸‹æ–‡ï¼Œç”¨äº NER æå–
        full_context_text = f"{formatted_history}\nç”¨æˆ·ï¼š{current_question}"

        # 1. å®ä½“æå– (å¯¹å®Œæ•´çš„ä¸Šä¸‹æ–‡è¿›è¡Œæå–)
        entities = ner_extractor.extract_entities(full_context_text)
        print(f"ğŸ” [NER]: {entities}")

        # 2. çŸ¥è¯†å›¾è°±æ£€ç´¢ (RAG)
        kg_context = ""
        if entities:
            contexts = []
            for ent in entities:
                # éå†æ‰€æœ‰å®ä½“è¿›è¡Œæ£€ç´¢
                info = kg_retriever.query_entity_context(ent)
                if info: contexts.append(info)
            kg_context = "\n\n".join(contexts)

        print(f"ğŸ“š [Graph Context]:\n{kg_context[:200]}..." if kg_context else "ğŸ“š [Graph Context]: None")

        # 3. æ„é€  Prompt

        # å¼•å¯¼æ€§æŒ‡ä»¤ï¼šæŒ‡å¯¼ LLM ä¼˜å…ˆå‚è€ƒçŸ¥è¯†åº“å’Œå†å²ï¼Œå¹¶è¿›è¡Œè¿½é—®
        guiding_instruction = (
            "ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„åŒ»ç–—åŠ©æ‰‹ï¼Œè¯·ä¼˜å…ˆå‚è€ƒæä¾›çš„ã€çŸ¥è¯†åº“ä¿¡æ¯ã€‘å’Œã€å†å²å¯¹è¯ã€‘æ¥å›ç­”å½“å‰ç”¨æˆ·çš„é—®é¢˜ã€‚ "
            "å¦‚æœã€çŸ¥è¯†åº“ä¿¡æ¯ã€‘ä¸è¶³ä»¥å¾—å‡ºç»“è®ºï¼Œæˆ–è€…ç”¨æˆ·çš„æè¿°åƒæ˜¯ä¸€ä¸ªæ–°ç—…ç—‡ï¼Œè¯·**ç¤¼è²Œåœ°è¿›è¡Œè¿½é—®**ï¼Œä¾‹å¦‚è¯¢é—®æ›´å…·ä½“çš„ç—‡çŠ¶ã€æŒç»­æ—¶é—´ã€å‘ä½œé¢‘ç‡æˆ–è¿‘æœŸæ´»åŠ¨ï¼Œä»¥æä¾›æ›´å‡†ç¡®çš„æŒ‡å¯¼ã€‚"
        )

        input_context = f"ã€çŸ¥è¯†åº“ä¿¡æ¯ã€‘:\n{kg_context or 'æ— ç›¸å…³ä¿¡æ¯'}\n\nã€å†å²å¯¹è¯ã€‘:\n{formatted_history or 'é¦–æ¬¡å¯¹è¯'}"

        prompt = prompter.generate_prompt(
            instruction=guiding_instruction,
            # å°†å½“å‰ç”¨æˆ·é—®é¢˜ä½œä¸º Input çš„ä¸€éƒ¨åˆ†
            input=f"å½“å‰ç”¨æˆ·é—®é¢˜ï¼š{current_question}\n\n{input_context}"
        )

        # 4. ç”Ÿæˆ
        inputs = tokenizer(prompt, return_tensors="pt").to(device)
        inputs.pop("token_type_ids", None)

        output_ids = model.generate(
            **inputs,
            generation_config=GenerationConfig(
                temperature=0.1, top_p=0.75, top_k=40, num_beams=1, max_new_tokens=512
            )
        )
        response = tokenizer.decode(output_ids[0], skip_special_tokens=True)
        final_answer = prompter.get_response(response)

        # 5. æ›´æ–°å†å²è®°å½•
        new_history = history + [(current_question, final_answer)]

        return final_answer, new_history

    # ---- å¯åŠ¨ Gradio ç•Œé¢ ----
    if gradio:
        with gr.Blocks(title="Med-GRAG System") as demo:
            gr.Markdown("<h1>ğŸ§  Med-GRAG å¤šè½®å¯¹è¯ç³»ç»Ÿ</h1>")
            gr.Markdown("Chinese-LLaMA-2 + LoRA + Qwen-NER + Custom Medical Graph (æ”¯æŒå¤šè½®ä¸Šä¸‹æ–‡ç†è§£)")

            # gr.State å­˜å‚¨å†å²å¯¹è¯ [(ç”¨æˆ·é—®, åŠ©æ‰‹ç­”), ...]
            history_state = gr.State([])

            chatbot = gr.Chatbot(label="ğŸ©º å¯¹è¯è®°å½•", height=400)
            msg = gr.Textbox(label="è¾“å…¥ä½ çš„é—®é¢˜æˆ–ç—‡çŠ¶æè¿°ï¼š")

            clear = gr.Button("ğŸ—‘ï¸ æ¸…é™¤å†å²è®°å½•")

            def respond(message, chat_history_list):
                # è°ƒç”¨ evaluateï¼Œè¿”å› æœ€ç»ˆå›ç­” å’Œ æ›´æ–°åçš„å†å²åˆ—è¡¨
                final_answer, new_history = evaluate(message, chat_history_list)

                # è¿”å›ç»™ Gradio ç»„ä»¶
                return new_history, "", new_history

            # ç»‘å®šäº‹ä»¶
            msg.submit(
                respond,
                [msg, history_state],
                [chatbot, msg, history_state],
                queue=False
            )

            # æ¸…é™¤å†å²è®°å½•äº‹ä»¶ï¼šæ¸…ç©º Chatbot å’Œ State
            clear.click(lambda: ([], []), None, [chatbot, history_state], queue=False)

        demo.launch(server_name="0.0.0.0", share=False)
    else:
        # é Gradio æ¨¡å¼çš„ç®€å•å¤šè½®æµ‹è¯•
        history = []
        q1 = "æˆ‘æœ€è¿‘æ€»æ˜¯å¤´ç—›ï¼Œè€Œä¸”æœ‰ç‚¹é«˜è¡€å‹ï¼Œè¯·é—®é¥®é£Ÿä¸Šè¦æ³¨æ„ä»€ä¹ˆï¼Ÿ"
        print(f"\n======== ç¬¬ä¸€æ¬¡å¯¹è¯ ========\nQ1: {q1}")
        a1, history = evaluate(q1, history)
        print(f"A1: {a1}")

        q2 = "é‚£åƒé˜¿å¸åŒ¹æ—å¯ä»¥å—ï¼Ÿè¿™ä¸ªè¯æœ‰ä»€ä¹ˆä½œç”¨ï¼Ÿ"
        print(f"\n======== ç¬¬äºŒæ¬¡å¯¹è¯ (RAG å¼•ç”¨ Q1 çš„å®ä½“) ========\nQ2: {q2}")
        a2, history = evaluate(q2, history)
        print(f"A2: {a2}")


if __name__ == "__main__":
    fire.Fire(main)